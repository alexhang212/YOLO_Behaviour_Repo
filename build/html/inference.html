<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Visualization and inference &mdash; YOLO-Behaviour 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
    <link rel="canonical" href="https://alexhang212.github.io/YOLO_Behaviour_Repo/inference.html" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=e031e9a9"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Validation and grid search" href="validation.html" />
    <link rel="prev" title="Model training" href="training.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            YOLO-Behaviour
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction and installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="annotation.html">Image annotation</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Model training</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Visualization and inference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="validation.html">Validation and grid search</a></li>
<li class="toctree-l1"><a class="reference internal" href="human.html">Human in the loop methods</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">YOLO-Behaviour</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Visualization and inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/inference.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="visualization-and-inference">
<span id="inference"></span><h1>Visualization and inference<a class="headerlink" href="#visualization-and-inference" title="Permalink to this heading"></a></h1>
<p>Now that you have a model trained, its time to see how good the model is. I have prepared two scripts for this, first is just for visualizing, and the second one is for inference, where the results will be saved as a pickle or a csv.</p>
<p><h1> Visualization </h1></p>
<p>The script to visualize results is <code class="docutils literal notranslate"><span class="pre">Code/3_VisualizeResults.py</span></code>. This script requires the trained model and a sample video. Here are the arguments:</p>
<ul class="simple">
<li><p>--Video: Path to the sample video</p></li>
<li><p>--Weight: Path to YOLO weight file (See <a class="reference internal" href="training.html#training"><span class="std std-ref">Model training</span></a>. for more details)</p></li>
<li><p>--Start: Start frame</p></li>
<li><p>--Frames: Total number of frames, -1 means all frames</p></li>
</ul>
<p>To run this on the Jay sample video, you can run this in the terminal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">Code</span><span class="o">/</span><span class="mi">3</span><span class="n">_VisualizeResults</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">Video</span> <span class="s2">&quot;./Data/JaySampleData/Jay_Sample.mp4&quot;</span> <span class="o">--</span><span class="n">Weight</span>  <span class="s2">&quot;./Data/Weights/JayBest.pt&quot;</span> <span class="o">--</span><span class="n">Start</span> <span class="mi">0</span> <span class="o">--</span><span class="n">Frames</span> <span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>This should then launch a window where the video will be playing, with detected bounding boxes drawn on top. It will also save the results as a video in the current directory, called <code class="docutils literal notranslate"><span class="pre">YOLO_Sample.mp4</span></code></p>
<img alt="_images/JayOutput.png" src="_images/JayOutput.png" />
<p><h1> Inference </h1>
If you are happy with the results, you can then proceed to run inference in a whole video. The script for this is <code class="docutils literal notranslate"><span class="pre">Code/4_RunInference.py</span></code>, which takes in a video and outputs results as a pickle or csv. The sample scripts only does this for 1 video, so I highly encourage you to extend the script to do multiple videos! Here are the arguments:</p>
<ul class="simple">
<li><p>--Video: Path to the sample video</p></li>
<li><p>--Weight: Path to YOLO weight file (See <a class="reference internal" href="training.html#training"><span class="std std-ref">Model training</span></a>. for more details)</p></li>
<li><p>--Output: Output type, either “csv” or “pickle”</p></li>
</ul>
<p>To run this on the Jay sample video, you can run this in the terminal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">Code</span><span class="o">/</span><span class="mi">4</span><span class="n">_RunInference</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">Video</span> <span class="s2">&quot;./Data/JaySampleData/Jay_Sample.mp4&quot;</span> <span class="o">--</span><span class="n">Weight</span>  <span class="s2">&quot;./Data/Weights/JayBest.pt&quot;</span> <span class="o">--</span><span class="n">Output</span> <span class="n">csv</span>
</pre></div>
</div>
<p>This will run inference and save the results in a csv, with the same name as the video, in the video’s directory.</p>
<p><h1> Data Formats </h1></p>
<p>If you chose to save it as a pickle, the data is actually saved as a big python dictionary. You can load it back using the pickle library within python, and access the data like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">###This is within python!!! Not the command line</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;Data/JaySampleData/Jay_Sample_YOLO.pkl&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
<p>The dictionary is structured as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="n">frame_number</span><span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;Class&quot;</span><span class="p">:</span> <span class="p">[</span><span class="nb">list</span> <span class="n">of</span> <span class="n">classes</span> <span class="n">detected</span><span class="p">],</span>
    <span class="s2">&quot;conf&quot;</span><span class="p">:</span> <span class="p">[</span><span class="nb">list</span> <span class="n">of</span> <span class="n">confidence</span> <span class="n">scores</span><span class="p">],</span>
    <span class="s2">&quot;bbox&quot;</span><span class="p">:</span> <span class="p">[</span><span class="nb">list</span> <span class="n">of</span> <span class="n">bounding</span> <span class="n">boxes</span><span class="p">]}</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Within the dictionary, each frame number is a key, which can be used to return detections from that frame, e.g. <code class="docutils literal notranslate"><span class="pre">data[0]</span></code> will return the detections from the first frame.</p>
<p>Within each frame, there is another dictionary, with keys “Class”, “conf” and “bbox”. These are strings of the classes detected, the confidence scores and the bounding boxes respectively. The bounding boxes are in the format of [x1, y1, x2, y2], where x1, y1 is the top left corner, and x2, y2 is the bottom right corner. If there are multiple bounding boxes detected for a given frame, the length of each list will be larger than 1. If nothing was detected in a frame, all the lists will be empty.</p>
<p>If you decided to output as a csv, this is what the data looks like:</p>
<img alt="_images/csvSample.png" src="_images/csvSample.png" />
<dl class="simple">
<dt>Here are the columns:</dt><dd><ul class="simple">
<li><p>Frame: Frame number</p></li>
<li><p>Behaviour: The type of behaviour detected</p></li>
<li><p>Confidence: The confidence score of the detection</p></li>
<li><p>BBox_xmin: The x coordinate of the top left corner</p></li>
<li><p>BBox_ymin: The y coordinate of the top left corner</p></li>
<li><p>BBox_xmax: The x coordinate of the bottom right corner</p></li>
<li><p>BBox_ymax: The y coordinate of the bottom right corner</p></li>
</ul>
</dd>
</dl>
<p><h1> Event Detection </h1>
After getting the detections from YOLO, the next step is to group these detections into events. For this, I use a simple tracking algorithm called <a class="reference external" href="https://github.com/abewley/sort">SORT</a> (Before running this step, make sure you cloned the SORT repositories under <code class="docutils literal notranslate"><span class="pre">Reposositories</span></code>, see <a class="reference internal" href="intro.html#install"><span class="std std-ref">Introduction and installation</span></a>).
You dont really need to know how the algorithm works, but basically it groups the detected boxes together based on physical and temporal proximity, then assign an ID to each “track”.
For example, if you have bounding box detection of a bird flying across the screen, one might use a tracking algorithm to track and assign an ID to the bird across frames.
Here, we use the same type of algorithm to assign detections into “behavioural events”.</p>
<p>The script to do this is <code class="docutils literal notranslate"><span class="pre">Code/5_GetEvents.py</span></code>, but before this,you will need to have the detections output as pickle format (see above), and also define the hyper-paramters for the alogrithm is a seperate json file.</p>
<p>Here is a sample hyper parameter file, you can also find this in <code class="docutils literal notranslate"><span class="pre">./Data/JaySampleData/Jay_Sample_HyperParameters.json</span></code></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;Eat&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;min_duration&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;YOLO_Threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.1</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;max_age&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">21.0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;min_hits&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;iou_threshold&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<dl class="simple">
<dt>The json basically stores a python dictionary, with the first level being the name of the behaviour (It needs to be the same as the YOLO output), then each behaviour has a set of hyperparamters.</dt><dd><ul class="simple">
<li><p>min_duration: The minimum duration of a track to be considered an event</p></li>
<li><p>YOLO_Threshold: The confidence threshold from YOLO to consider a detection</p></li>
<li><p>max_age: The maximum number of frames a track can be inactive before being deleted</p></li>
<li><p>min_hits: The minimum number of hits to create a track</p></li>
<li><p>iou_threshold: The overlap in bounding boxes to consider them the same track</p></li>
</ul>
</dd>
</dl>
<p>The most important hyper-paramters are probably <code class="docutils literal notranslate"><span class="pre">min_duration</span></code> and <code class="docutils literal notranslate"><span class="pre">YOLO_Threshold</span></code>, which will vary depending on the type of behaviour and video you have.
For example, an increased minimum duration or a higher YOLO threshold will be useful to filter out super short detections/ wrong detections that might be false positives, but ofcourse depends on the length of the behaviour of interest.</p>
<p>Here, I defined the best hyperparamters for the Jay dataset, based on a grid search optimization (See next section) as an example.
If you are applying this to a new dataset, you can definitely play around with the parameters a little first to get a feel of how each paramter differs, since running a whole optimization algorithm might be a little daunting at first.</p>
<p>To run the event detection on the Jay sample video, you can run this in the terminal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">Code</span><span class="o">/</span><span class="mi">5</span><span class="n">_GetEvents</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">Detection</span> <span class="s2">&quot;./Data/JaySampleData/Jay_Sample_YOLO.pkl&quot;</span> <span class="o">--</span><span class="n">Param</span>  <span class="s2">&quot;./Data/JaySampleData/Jay_Sample_HyperParam.json&quot;</span>
</pre></div>
</div>
<dl class="simple">
<dt>The parameters are:</dt><dd><ul class="simple">
<li><p>--Detection: Path to the YOLO detections in pickle format</p></li>
<li><p>--Param: Path to the hyperparameter json file</p></li>
</ul>
</dd>
</dl>
<p>After running the script, it will output the detected events as a csv file, with the same name as the video, in the video’s directory.</p>
<img alt="_images/EventsCSV.png" src="_images/EventsCSV.png" />
<dl class="simple">
<dt>The columns are the following:</dt><dd><ul class="simple">
<li><p>Behaviour: The name of behaviour</p></li>
<li><p>StartFrame: Start frame of the behaviour</p></li>
<li><p>EndFrame: End frame of the behaviour</p></li>
</ul>
</dd>
</dl>
<p>One could then use this information to further analysis, for example to calculate duration of events, or converting the frame numbers to actual time, based on the frame rate of the video.</p>
<p>Next section, I will go through the model validation and optimization using grid search, which might need a bit more customization and coding for new datasets, since it will depend on the format of your manual annotation!
However, I do really encourage you to validate the model before you apply it to process data on a study (even if the detections looks really good from the videos!).</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="training.html" class="btn btn-neutral float-left" title="Model training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="validation.html" class="btn btn-neutral float-right" title="Validation and grid search" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Alex Chan Hoi Hang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>